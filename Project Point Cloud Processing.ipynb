{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project, you will be asked to implement [PointNet](https://arxiv.org/abs/1612.00593) architecture and train a classification network (left) and a segmentation network (middle).\n",
    "![title](img/cls_sem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Points\n",
    "* Task 1.1 - 5\n",
    "* Task 1.2 - 5\n",
    "* Task 2.1 - 10\n",
    "* Task 2.2 - 5\n",
    "* Task 2.3 - 5\n",
    "* Task 2.4 - 5\n",
    "* Task 2.5 - 5\n",
    "* Task 2.6 - 10\n",
    "* Task 2.7 - 5\n",
    "* Task 2.8 - 10\n",
    "* Task 2.9 - 10\n",
    "* Task 2.10 - 5 \n",
    "* Task 2.11 - 5\n",
    "* Task 2.12 - 5\n",
    "* Task 2.13 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import dataset # custom dataset for ModelNet10 and ShapeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we write the point cloud as $X\\in\\mathbb{R}^{N\\times 3}$. While in programming, we use `B x 3 x N` layout, where `B` is the batch-size and `N` is the number of points in a single point cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Jitter the position of each points by a zero mean Gaussian\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we transform $X$ by $X \\leftarrow X + \\mathcal{N}(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomJitter(object):\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## hint: useful function `torch.randn` and `torch.randn_like`\n",
    "        ## TASK 1.1\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the jittered point cloud of layout `3 x N`.\n",
    "#       torch.randn mean=0, variance=1\n",
    "        distribution = torch.empty(data.shape).normal_(mean=0,std=self.sigma)\n",
    "        data = data + distribution\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test your transform here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomJitter = RandomJitter(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3520,  0.5141,  0.2169, -2.6240, -0.4214,  0.4747, -0.0183, -0.3933,\n",
       "          0.7387, -0.8958, -0.2121,  1.9848,  0.2685, -1.2039,  1.6044, -0.9679],\n",
       "        [-0.9044,  0.1868, -0.5167, -0.9403, -0.7028, -0.1019,  0.6148, -1.0564,\n",
       "         -0.8722,  1.3013, -1.1787, -0.0405,  1.0275,  0.3444, -1.9572,  0.9243],\n",
       "        [ 0.1299, -1.1597, -0.5881, -0.3834, -0.8663,  0.1068, -0.7399, -0.1256,\n",
       "         -1.3237, -1.5778, -0.0985, -0.5886,  1.2592, -0.8350, -0.1757,  2.6175]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomJitter.__call__(torch.randn(3,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rotate the object along the z-axis randomly\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we rotate all points along z-axis (up-axis) by a degree $\\theta$.\n",
    "\n",
    "\n",
    "Suppose $T$ is the transformation matrix,\n",
    "$$X\\leftarrow XT,$$\n",
    "where $$T=\\begin{bmatrix}\\cos\\theta & \\sin\\theta & 0 \\\\ -\\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[x,y,z]->[xcos\\theta-ysin\\theta, xsin\\theta-ycos\\theta, z]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomZRotate(object):\n",
    "    def __init__(self, degrees):\n",
    "        ## here `self.degrees` is a tuple (0, 360) which defines the range of degree\n",
    "        self.degrees = degrees\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## TASK 1.2\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the rotated point cloud of layout `3 x N`.\n",
    "        ##\n",
    "        ## The rotation is along z-axis, and the degree is uniformly distributed\n",
    "        ## between [0, 360]\n",
    "        ##\n",
    "        ## hint: useful function `torch.randn`ï¼Œ `torch.randn_like` and `torch.matmul`\n",
    "        ##\n",
    "        ## Notice:   \n",
    "        ## Different from its math notation `N x 3`, the input has size of `3 x N`\n",
    "        degree = torch.LongTensor(1).random_(self.degrees[0], self.degrees[1])\n",
    "#         print(degree)\n",
    "        d= 2 * np.pi * (degree/360)\n",
    "        T = torch.Tensor([[np.cos(d), np.sin(d),0],\n",
    "                         [-np.sin(d),np.cos(d),0],\n",
    "                         [0,0,1]])\n",
    "        data = torch.matmul(T,data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test your transform here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomZRotate = RandomZRotate((0,360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7704,  1.1422, -0.6398,  0.4064, -1.0655, -1.3393,  0.5662,  0.6398,\n",
       "         -0.1455,  1.3356, -1.0211, -1.1103,  0.8489,  0.1762,  0.7887, -0.5296],\n",
       "        [-1.2052,  0.0097,  0.2326, -0.9892,  0.0609,  1.3887,  1.2696,  0.2261,\n",
       "         -2.5262,  0.9990,  0.2980,  0.4682,  0.8535,  0.4311, -0.5221, -1.3735],\n",
       "        [ 1.3465,  1.0376,  0.5592,  1.5081, -0.5739, -0.3559,  0.8704,  0.0045,\n",
       "          0.2031, -0.1129, -0.6363, -2.5462, -0.4523,  0.2583, -0.3420, -0.0498]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomZRotate.__call__(torch.randn(3,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load dataset ModelNet10 for Point Cloud Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelNet10\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and label of shape `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may taske some time to download and pre-process the dataset.\n",
    "train_transform = Compose([RandomZRotate((0, 360)), RandomJitter(0.02)])\n",
    "train_cls_dataset = dataset.ModelNet(root='./ModelNet10', transform=train_transform, train=True)\n",
    "test_cls_dataset = dataset.ModelNet(root='./ModelNet10', train=False)\n",
    "train_cls_loader = data.DataLoader(\n",
    "    train_cls_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_cls_loader = data.DataLoader(\n",
    "    test_cls_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_cls_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShapeNet\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and target of shape `B x N`.\n",
    "\n",
    "Here is the list of categories:\n",
    "['Airplane', 'Bag', 'Cap', 'Car', 'Chair', 'Earphone', 'Guitar', 'Knife', 'Lamp', 'Laptop', 'Motorbike', 'Mug', 'Pistol', 'Rocket', 'Skateboard', 'Table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">classification for every point</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here as an example, we choose the cateogry 'Airplane'\n",
    "category = 'Airplane'\n",
    "train_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=True)\n",
    "test_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=False)\n",
    "train_seg_loader = data.DataLoader(\n",
    "    train_seg_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_seg_loader = data.DataLoader(\n",
    "    test_seg_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_seg_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PointNet Architecture (Read Section 4.2 and Appendix C)\n",
    "In this section, you will be asked to implement classification and segmentation step by step.\n",
    "![pointnet](img/pointnet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Joint Alignment Network \n",
    "This mini-network takes as input matrix of size $N \\times K$, and outputs a transformation matrix of size $K \\times K$. \n",
    "\n",
    "In programming, the input size of this module is `B x K x N` and output size is `B x K x K`.\n",
    "\n",
    "For the shared MLP, use structure like this `(FC(64), BN, ReLU, FC(128), BN, ReLU, FC(1024), BN, ReLU)`.\n",
    "\n",
    "For the MLP after global max pooling, use structure like this `(FC(512), BN, ReLU, FC(256), BN, ReLU, FC(K*K)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super(Transformation, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        ## TASK 2.1\n",
    "        \n",
    "        ## define your network layers here\n",
    "        ## shared mlp\n",
    "        ## input size: B x K x N\n",
    "        ## output size: B x 1024 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here. Why?\n",
    "        # depthwise groups=64? one filter will work on all of channels seperately, thus the number of output channels will be k*input_channels\n",
    "        self.share_mlp = nn.Sequential(nn.Conv1d(k, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 64*2,1, stride=1),\n",
    "                                      nn.BatchNorm1d(64*2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64*2, 64*16, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64*16),\n",
    "                                      nn.ReLU())\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x (K*K)\n",
    "        self.mlp = nn.Sequential(nn.Linear(64*16, 64*8), \n",
    "                                 nn.BatchNorm1d(64*8),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64*8, 64*4),\n",
    "                                 # batch size should be larger than 1, otherwise there will have an error\n",
    "                                 nn.BatchNorm1d(64*4),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64*4, k**2))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, K, N = x.shape # batch-size, dim, number of points\n",
    "        ## TASK 2.1\n",
    "        self.k = K\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.share_mlp(x)\n",
    "#         ## global max pooling\n",
    "#         # input - B x 1024 x N\n",
    "#         # output - B x 1024\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "#         ## mlp\n",
    "#         # input - B x 1024\n",
    "#         # output - B x (K*K)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        ## reshape the transformation matrix to B x K x K\n",
    "        identity = torch.eye(self.k, device=x.device)\n",
    "        x = x.view(B, self.k, self.k) + identity[None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = Transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation(torch.randn(5,3,8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regularization Loss\n",
    "$$L_{reg}=\\|I-TT^\\intercal\\|^2_F$$\n",
    "\n",
    "The output of `Transformation` network is of size `B x K x K`. The module `OrthoLoss` has no trainable parameters, only computes this norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrthoLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## hint: useful function `torch.bmm` and `torch.matmul`\n",
    "#         Performs a batch matrix-matrix product of matrices\n",
    "        ## TASK 2.2\n",
    "        ## compute the matrix product\n",
    "        prod = torch.bmm(x,torch.transpose(x, 1, 2))\n",
    "        norm = torch.norm(prod - torch.eye(x.shape[1], device=x.device)[None], dim=(1,2))\n",
    "        return norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthoLoss = OrthoLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0467623999999995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.7844)**2+(-1.3648)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7844, -1.3648],\n",
      "         [-0.6096, -1.7659]],\n",
      "\n",
      "        [[-1.1674, -0.6116],\n",
      "         [-0.6308, -0.4505]]])\n",
      "tensor([[[-1.7844, -0.6096],\n",
      "         [-1.3648, -1.7659]],\n",
      "\n",
      "        [[-1.1674, -0.6308],\n",
      "         [-0.6116, -0.4505]]])\n",
      "tensor([[[5.0470, 3.4978],\n",
      "         [3.4978, 3.4899]],\n",
      "\n",
      "        [[1.7370, 1.0120],\n",
      "         [1.0120, 0.6009]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.2588)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthoLoss(torch.randn(2,2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Network\n",
    "In this subsection, you will be asked to implement the feature network (the top branch).\n",
    "\n",
    "Local features are a matrix of size `B x 64 x N`, which will be used in the segmentation task.\n",
    "\n",
    "Global features are a matrix of size `B x 1024`, which will be used in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self, alignment=False):\n",
    "        super(Feature, self).__init__()\n",
    "        \n",
    "        self.alignment = alignment\n",
    "        \n",
    "        ## `input_transform` calculates the input transform matrix of size `3 x 3`\n",
    "        if self.alignment:\n",
    "            self.input_transform = Transformation(3)\n",
    "        \n",
    "        ## TASK 2.3\n",
    "        ## define your network layers here\n",
    "        ## local feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 3 x N\n",
    "        ## output size: B x 64 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here.\n",
    "        self.local_feature = nn.Sequential(nn.Conv1d(3, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU())\n",
    "        ## `feature_transform` calculates the feature transform matrix of size `64 x 64`\n",
    "        if self.alignment:\n",
    "            self.feature_transform = Transformation(64)\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## define your network layers here\n",
    "        ## global feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 64 x N\n",
    "        ## output size: B x 1024 x N\n",
    "        self.global_feature = nn.Sequential(\n",
    "                              nn.Conv1d(64, 64*2,1, stride=1),\n",
    "                              nn.BatchNorm1d(64*2),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(64*2, 64*16, 1, stride=1),\n",
    "                              nn.BatchNorm1d(64*16),\n",
    "                              nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,K,N = x.shape\n",
    "        ## apply the input transform\n",
    "        if self.alignment:\n",
    "            transform = self.input_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the input transform\n",
    "            x = torch.bmm(transform,x)\n",
    "    \n",
    "\n",
    "        ## TASK 2.3\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 64 x N\n",
    "        x = self.local_feature(x)\n",
    "        \n",
    "        if self.alignment:\n",
    "            transform = self.feature_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the feature transform\n",
    "            x = torch.bmm(transform,x)\n",
    "        else:\n",
    "            ## do not modify this line\n",
    "            transform = None\n",
    "        \n",
    "        local_feature = x\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 64 x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.global_feature(x)\n",
    "        \n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## global max pooling\n",
    "        # input - B x 1024 x N\n",
    "        # output - B x 1024\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.view(-1,1024)\n",
    "        global_feature = x\n",
    "        ## summary:\n",
    "        ## global_feature: B x 1024\n",
    "        ## local_feature: B x 64 x N\n",
    "        ## transform: B x K x K\n",
    "        return global_feature, local_feature, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = Feature(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = feature(torch.randn(3,3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transform: B x K x K the size of transform??\n",
    "result[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification Network\n",
    "In this network, you will use the global features generated by the `Feature` network defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Classification, self).__init__()\n",
    "                \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "        \n",
    "        ## TASK 2.6\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x num_classes=10\n",
    "        self.mlp = nn.Sequential(nn.Linear(64*16, 64*8), \n",
    "                         nn.BatchNorm1d(64*8),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64*8, 64*4),\n",
    "                         # batch size should be larger than 1, otherwise there will have an error\n",
    "                         nn.BatchNorm1d(64*4),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64*4, 64*2),\n",
    "                         nn.BatchNorm1d(64*2),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64*2, 64),\n",
    "                         nn.BatchNorm1d(64),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the global feature matrix\n",
    "        # the size of global_feature: B x 1024\n",
    "        # here we don't use local feature matrix\n",
    "        x, _, trans = self.feature(x)\n",
    "        \n",
    "        ## TASK 2.6\n",
    "        ## forward of mlp\n",
    "        # input - B x 1024\n",
    "        # output - B x num_classes        \n",
    "        x = self.mlp(x)\n",
    "        ## x: B x num_classes\n",
    "        ## trans: B x K x K\n",
    "        # add a sigmoid\n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes=10\n",
    "classification = Classification(10)\n",
    "result = classification(torch.randn(3,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Train this network on ModelNet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "def train_cls(train_loader, test_loader, network, optimizer, epochs, scheduler):\n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            \n",
    "            ## TASK 2.7\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            loss = loss = nn.CrossEntropyLoss()(output,label)\n",
    "            ##########\n",
    "            \n",
    "            ## regularizer\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001\n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(label).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/len(train_loader.dataset) * 100))\n",
    "        \n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "    \n",
    "                ## TASK 2.7\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.CrossEntropyLoss()(output,label)\n",
    "                ##########\n",
    "\n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001\n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/len(test_loader.dataset) * 100))\n",
    "        print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.5480\n",
      "Average Train Loss: 1.3437; Train Acc: 60.9622\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.2261\n",
      "Average Test Loss: 1.1003; Test Acc: 68.2819\n",
      "-------------------------------------------\n",
      "Epoch:[02/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9268\n",
      "Average Train Loss: 0.9260; Train Acc: 72.5883\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.6264\n",
      "Average Test Loss: 1.3723; Test Acc: 55.0661\n",
      "-------------------------------------------\n",
      "Epoch:[03/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.0243\n",
      "Average Train Loss: 0.8142; Train Acc: 75.4197\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.3723\n",
      "Average Test Loss: 0.7383; Test Acc: 73.7885\n",
      "-------------------------------------------\n",
      "Epoch:[04/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9356\n",
      "Average Train Loss: 0.6601; Train Acc: 80.2305\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1054\n",
      "Average Test Loss: 0.7166; Test Acc: 72.0264\n",
      "-------------------------------------------\n",
      "Epoch:[05/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0837\n",
      "Average Train Loss: 0.5766; Train Acc: 81.9845\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.3380\n",
      "Average Test Loss: 0.8689; Test Acc: 71.8062\n",
      "-------------------------------------------\n",
      "Epoch:[06/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5059\n",
      "Average Train Loss: 0.5443; Train Acc: 82.6359\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0899\n",
      "Average Test Loss: 1.2890; Test Acc: 67.0705\n",
      "-------------------------------------------\n",
      "Epoch:[07/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.6381\n",
      "Average Train Loss: 0.4810; Train Acc: 85.1165\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.2949\n",
      "Average Test Loss: 0.5682; Test Acc: 82.0485\n",
      "-------------------------------------------\n",
      "Epoch:[08/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.8913\n",
      "Average Train Loss: 0.4072; Train Acc: 87.3716\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0798\n",
      "Average Test Loss: 0.4699; Test Acc: 83.9207\n",
      "-------------------------------------------\n",
      "Epoch:[09/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7210\n",
      "Average Train Loss: 0.3892; Train Acc: 87.9479\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0694\n",
      "Average Test Loss: 0.6053; Test Acc: 77.0925\n",
      "-------------------------------------------\n",
      "Epoch:[10/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1481\n",
      "Average Train Loss: 0.3694; Train Acc: 88.3237\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0257\n",
      "Average Test Loss: 0.4302; Test Acc: 85.4626\n",
      "-------------------------------------------\n",
      "Epoch:[11/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2345\n",
      "Average Train Loss: 0.3499; Train Acc: 88.7497\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0581\n",
      "Average Test Loss: 0.4831; Test Acc: 83.2599\n",
      "-------------------------------------------\n",
      "Epoch:[12/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2959\n",
      "Average Train Loss: 0.3120; Train Acc: 89.9273\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0213\n",
      "Average Test Loss: 0.4056; Test Acc: 87.5551\n",
      "-------------------------------------------\n",
      "Epoch:[13/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6499\n",
      "Average Train Loss: 0.3042; Train Acc: 90.5537\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.04680\n",
      "Average Test Loss: 0.7614; Test Acc: 74.7797\n",
      "-------------------------------------------\n",
      "Epoch:[14/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1355\n",
      "Average Train Loss: 0.3127; Train Acc: 90.4034\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.02497\n",
      "Average Test Loss: 0.3726; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[15/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.6302\n",
      "Average Train Loss: 0.2652; Train Acc: 92.0070\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0267\n",
      "Average Test Loss: 0.3506; Test Acc: 88.2159\n",
      "-------------------------------------------\n",
      "Epoch:[16/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0657\n",
      "Average Train Loss: 0.2782; Train Acc: 91.2804\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0103\n",
      "Average Test Loss: 0.3531; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[17/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1434\n",
      "Average Train Loss: 0.2761; Train Acc: 91.5059\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0335\n",
      "Average Test Loss: 0.4806; Test Acc: 82.4890\n",
      "-------------------------------------------\n",
      "Epoch:[18/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0692\n",
      "Average Train Loss: 0.2657; Train Acc: 91.7063\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0087\n",
      "Average Test Loss: 0.3480; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[19/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2416\n",
      "Average Train Loss: 0.2331; Train Acc: 92.4330\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0097\n",
      "Average Test Loss: 0.3180; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[20/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3339\n",
      "Average Train Loss: 0.2442; Train Acc: 92.2576\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0188\n",
      "Average Test Loss: 0.3438; Test Acc: 87.8855\n",
      "-------------------------------------------\n",
      "Epoch:[21/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.9547\n",
      "Average Train Loss: 0.1955; Train Acc: 93.8862\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0156\n",
      "Average Test Loss: 0.3349; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[22/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.3243\n",
      "Average Train Loss: 0.1670; Train Acc: 94.5628\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0116\n",
      "Average Test Loss: 0.3836; Test Acc: 86.1233\n",
      "-------------------------------------------\n",
      "Epoch:[23/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0411\n",
      "Average Train Loss: 0.1621; Train Acc: 94.9887\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0045\n",
      "Average Test Loss: 0.3075; Test Acc: 88.4361\n",
      "-------------------------------------------\n",
      "Epoch:[24/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2191\n",
      "Average Train Loss: 0.1462; Train Acc: 95.1641\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0053\n",
      "Average Test Loss: 0.3018; Test Acc: 89.5374\n",
      "-------------------------------------------\n",
      "Epoch:[25/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0945\n",
      "Average Train Loss: 0.1469; Train Acc: 95.0890\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0144\n",
      "Average Test Loss: 0.3515; Test Acc: 87.7753\n",
      "-------------------------------------------\n",
      "Epoch:[26/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1331\n",
      "Average Train Loss: 0.1507; Train Acc: 94.9887\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0137\n",
      "Average Test Loss: 0.3264; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[27/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2867\n",
      "Average Train Loss: 0.1323; Train Acc: 95.7154\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0080\n",
      "Average Test Loss: 0.3218; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[28/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6262\n",
      "Average Train Loss: 0.1530; Train Acc: 94.9386\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00402\n",
      "Average Test Loss: 0.3102; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[29/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9512\n",
      "Average Train Loss: 0.1480; Train Acc: 95.2894\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0073\n",
      "Average Test Loss: 0.2788; Test Acc: 90.4185\n",
      "-------------------------------------------\n",
      "Epoch:[30/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0404\n",
      "Average Train Loss: 0.1474; Train Acc: 95.0388\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00971\n",
      "Average Test Loss: 0.2592; Test Acc: 92.0705\n",
      "-------------------------------------------\n",
      "Epoch:[31/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2924\n",
      "Average Train Loss: 0.1351; Train Acc: 95.4397\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0102\n",
      "Average Test Loss: 0.3056; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[32/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1687\n",
      "Average Train Loss: 0.1244; Train Acc: 95.7655\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0049\n",
      "Average Test Loss: 0.2627; Test Acc: 91.0793\n",
      "-------------------------------------------\n",
      "Epoch:[33/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3532\n",
      "Average Train Loss: 0.1355; Train Acc: 95.3896\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0035\n",
      "Average Test Loss: 0.2949; Test Acc: 89.9780\n",
      "-------------------------------------------\n",
      "Epoch:[34/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4055\n",
      "Average Train Loss: 0.1241; Train Acc: 95.4397\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00492\n",
      "Average Test Loss: 0.3309; Test Acc: 88.5463\n",
      "-------------------------------------------\n",
      "Epoch:[35/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0491\n",
      "Average Train Loss: 0.1117; Train Acc: 96.0661\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00620\n",
      "Average Test Loss: 0.3135; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[36/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2245\n",
      "Average Train Loss: 0.1289; Train Acc: 95.3646\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [908/908] Loss: 0.00358\n",
      "Average Test Loss: 0.3067; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[37/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5369\n",
      "Average Train Loss: 0.1196; Train Acc: 95.9659\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00630\n",
      "Average Test Loss: 0.4259; Test Acc: 86.8943\n",
      "-------------------------------------------\n",
      "Epoch:[38/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9147\n",
      "Average Train Loss: 0.1356; Train Acc: 95.7905\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0016\n",
      "Average Test Loss: 0.3755; Test Acc: 87.0044\n",
      "-------------------------------------------\n",
      "Epoch:[39/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1215\n",
      "Average Train Loss: 0.1306; Train Acc: 95.8657\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0018\n",
      "Average Test Loss: 0.2585; Test Acc: 91.6300\n",
      "-------------------------------------------\n",
      "Epoch:[40/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0555\n",
      "Average Train Loss: 0.1151; Train Acc: 96.1914\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0057\n",
      "Average Test Loss: 0.8041; Test Acc: 77.6432\n",
      "-------------------------------------------\n",
      "Epoch:[41/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0114\n",
      "Average Train Loss: 0.0934; Train Acc: 96.5923\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0025\n",
      "Average Test Loss: 0.3150; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[42/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0867\n",
      "Average Train Loss: 0.0747; Train Acc: 97.2689\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0045\n",
      "Average Test Loss: 0.2883; Test Acc: 90.4185\n",
      "-------------------------------------------\n",
      "Epoch:[43/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0119\n",
      "Average Train Loss: 0.0803; Train Acc: 97.3190\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0032\n",
      "Average Test Loss: 0.2909; Test Acc: 90.8590\n",
      "-------------------------------------------\n",
      "Epoch:[44/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0790\n",
      "Average Train Loss: 0.0826; Train Acc: 97.0433\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0035\n",
      "Average Test Loss: 0.2641; Test Acc: 90.6388\n",
      "-------------------------------------------\n",
      "Epoch:[45/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6426\n",
      "Average Train Loss: 0.0894; Train Acc: 96.9932\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00199\n",
      "Average Test Loss: 0.2414; Test Acc: 92.4009\n",
      "-------------------------------------------\n",
      "Epoch:[46/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1905\n",
      "Average Train Loss: 0.0719; Train Acc: 97.5946\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00317\n",
      "Average Test Loss: 0.2913; Test Acc: 90.4185\n",
      "-------------------------------------------\n",
      "Epoch:[47/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0419\n",
      "Average Train Loss: 0.0819; Train Acc: 96.9181\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00172\n",
      "Average Test Loss: 0.2935; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[48/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 3.9723\n",
      "Average Train Loss: 0.0940; Train Acc: 97.0935\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00139\n",
      "Average Test Loss: 0.2595; Test Acc: 91.8502\n",
      "-------------------------------------------\n",
      "Epoch:[49/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6841\n",
      "Average Train Loss: 0.0833; Train Acc: 96.9682\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00087\n",
      "Average Test Loss: 0.3255; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[50/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1919\n",
      "Average Train Loss: 0.0770; Train Acc: 97.1686\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0062\n",
      "Average Test Loss: 0.2772; Test Acc: 90.6388\n",
      "-------------------------------------------\n",
      "Epoch:[51/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0564\n",
      "Average Train Loss: 0.0730; Train Acc: 97.6196\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00210\n",
      "Average Test Loss: 0.2555; Test Acc: 91.7401\n",
      "-------------------------------------------\n",
      "Epoch:[52/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0838\n",
      "Average Train Loss: 0.0745; Train Acc: 97.4192\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00188\n",
      "Average Test Loss: 0.3050; Test Acc: 90.0881\n",
      "-------------------------------------------\n",
      "Epoch:[53/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1815\n",
      "Average Train Loss: 0.0769; Train Acc: 97.4442\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0014\n",
      "Average Test Loss: 0.3299; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[54/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0433\n",
      "Average Train Loss: 0.0714; Train Acc: 97.5946\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00335\n",
      "Average Test Loss: 0.3376; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[55/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3656\n",
      "Average Train Loss: 0.0955; Train Acc: 96.7928\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00192\n",
      "Average Test Loss: 0.3108; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[56/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7058\n",
      "Average Train Loss: 0.0694; Train Acc: 97.7950\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00240\n",
      "Average Test Loss: 0.2688; Test Acc: 91.7401\n",
      "-------------------------------------------\n",
      "Epoch:[57/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.8267\n",
      "Average Train Loss: 0.0853; Train Acc: 97.4442\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00318\n",
      "Average Test Loss: 0.2533; Test Acc: 91.9604\n",
      "-------------------------------------------\n",
      "Epoch:[58/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1153\n",
      "Average Train Loss: 0.0674; Train Acc: 97.6196\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00327\n",
      "Average Test Loss: 0.3603; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[59/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4139\n",
      "Average Train Loss: 0.0692; Train Acc: 97.4192\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00376\n",
      "Average Test Loss: 0.3119; Test Acc: 89.4273\n",
      "-------------------------------------------\n",
      "Epoch:[60/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2259\n",
      "Average Train Loss: 0.0696; Train Acc: 97.3941\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00159\n",
      "Average Test Loss: 0.3438; Test Acc: 88.9868\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "network = Classification(10, alignment=True).cuda()\n",
    "epochs = 60 # you can change the value to a small number for debugging\n",
    "\n",
    "## TASK 2.8\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "# lr=0.001 is the default value\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)\n",
    "# # choose a lr scheduler\n",
    "# The learning rate is divided by 2 every 20 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "# start training\n",
    "train_cls(train_cls_loader, test_cls_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout can be considered to added to outcome the overfitting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Best test accuracy</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>2019-11-21</th>\n",
    "    <th>91.8502</th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Segmentation Network\n",
    "In this network, you will use the global features and local features generated by the `Feature` network defined above.\n",
    "\n",
    "The global feature matrix is of size `B x 1024` and the local feature matrix is of size `B x 64 x N`.\n",
    "\n",
    "They need to be stacked together to a new matrix of size `B x 1088 x n` (How?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Segmentation, self).__init__()\n",
    "               \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "\n",
    "        ## TASK 2.9\n",
    "        ## shared mlp\n",
    "        ## input size: B x 1088 x N\n",
    "        ## output size: B x num_classes x N\n",
    "        self.shared_mlp = nn.Sequential(nn.Conv1d(1088, 512, 1, stride=1),\n",
    "                              nn.BatchNorm1d(512),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(512, 256, 1, stride=1),\n",
    "                              nn.BatchNorm1d(256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(256, 128, 1, stride=1), \n",
    "                              nn.BatchNorm1d(128), \n",
    "                              nn.ReLU(), \n",
    "                              nn.Conv1d(128, num_classes, 1, stride=1),\n",
    "                              nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        g, l, trans = self.feature(x)\n",
    "        _,_,N = l.shape\n",
    "        ## TASK 2.10\n",
    "        # concat global features and local features to a single matrix\n",
    "        # g - B x 1024, global features\n",
    "        # l - B x 64 x N, local features\n",
    "        # x - B x 1088 x N, concatenated features\n",
    "        g = g.view(-1,1024,1)\n",
    "        g = torch.repeat_interleave(g, repeats=N, dim=2)\n",
    "        x = torch.cat((l, g),dim=1)\n",
    "        ## TASK 2.9\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 1088 x N\n",
    "        # output - B x num_classes x N  \n",
    "        x = self.shared_mlp(x)\n",
    "        \n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 10])\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 1024, 10])\n",
      "torch.Size([2, 1088, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3085, 0.3580, 0.2055, 0.3077, 0.1827, 0.2007, 0.2444, 0.1436,\n",
       "          0.2685, 0.2685],\n",
       "         [0.1180, 0.0993, 0.1350, 0.1060, 0.1260, 0.1242, 0.2211, 0.2402,\n",
       "          0.1519, 0.1605],\n",
       "         [0.1610, 0.1450, 0.1800, 0.1362, 0.1675, 0.2704, 0.0992, 0.2245,\n",
       "          0.1378, 0.1410],\n",
       "         [0.1881, 0.2316, 0.1807, 0.1233, 0.2530, 0.1836, 0.1218, 0.1965,\n",
       "          0.1605, 0.1336],\n",
       "         [0.2244, 0.1660, 0.2988, 0.3267, 0.2707, 0.2212, 0.3136, 0.1951,\n",
       "          0.2813, 0.2965]],\n",
       "\n",
       "        [[0.1676, 0.2040, 0.1574, 0.2405, 0.2126, 0.1016, 0.1611, 0.2288,\n",
       "          0.1782, 0.1799],\n",
       "         [0.1393, 0.1075, 0.1346, 0.1307, 0.0588, 0.0962, 0.1500, 0.1068,\n",
       "          0.1625, 0.1115],\n",
       "         [0.1833, 0.2392, 0.2997, 0.2236, 0.1437, 0.2246, 0.2608, 0.1987,\n",
       "          0.1709, 0.1799],\n",
       "         [0.1369, 0.2200, 0.0913, 0.1767, 0.2773, 0.3039, 0.1483, 0.1531,\n",
       "          0.1404, 0.2840],\n",
       "         [0.3730, 0.2293, 0.3171, 0.2286, 0.3076, 0.2737, 0.2799, 0.3125,\n",
       "          0.3480, 0.2447]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation = Segmentation(5)\n",
    "# B x 3 x N \n",
    "segmentation(torch.randn(2,3,10))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Calculating Intersection over Union (IoU) \n",
    "For 2D image, the IoU is calculated as follows,\n",
    "![iou](img/iou.png)\n",
    "\n",
    "How is it used in the literature of point clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 2.11\n",
    "# implement the helper functions to calculate the IoU\n",
    "def get_i_and_u(pred, target, num_classes):\n",
    "    \"\"\"Calculate intersection and union between pred and target.\n",
    "    \n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return i, u\n",
    "    i -- B x N binary matrix, intersection, i[b, n] equals 1 if and only if it is a true-positive.\n",
    "    u -- B x N binary matrix, union, u[b, n] equals 0 if and only if it is a true-negative\n",
    "    \"\"\"\n",
    "    ## TASK 2.11\n",
    "    ## calculate i and u here\n",
    "    ## hint: useful function `F.one_hot`    \n",
    "    ## hint: use element-wise logical tensor operation (`&` and `|`)\n",
    "    target_onehot = F.one_hot(target, num_classes=num_classes)\n",
    "    pre_onehot = F.one_hot(pred, num_classes=num_classes)\n",
    "    \n",
    "    i = torch.sum((target_onehot & pre_onehot).type(torch.float64), dim=1)\n",
    "    u = torch.sum((target_onehot | pre_onehot).type(torch.float64), dim=1)\n",
    "\n",
    "    return i, u\n",
    "\n",
    "def get_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculate IoU\n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return iou\n",
    "    iou -- B matrix, iou[b] is the IoU of b-th point cloud in this batch\n",
    "    \"\"\"\n",
    "    \n",
    "    ## use the helper function `i_and_u` defined above\n",
    "    i, u = get_i_and_u(pred, target, num_classes)\n",
    "    \n",
    "    ## TASK 2.11\n",
    "    ## calculate iou\n",
    "    iou = torch.sum(i,dim=1) / torch.sum(u,dim=1)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[2,3],[1,2]])\n",
    "b = torch.LongTensor([[2,3],[1,2]])\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = F.one_hot(a, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ = F.one_hot(b, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0]]])\n",
      "tensor([[[0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "print(a_) # target\n",
    "print(b_) # predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0., 0.]], dtype=torch.float64),\n",
       " tensor([[0., 0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_i_and_u(a, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_iou(a, b, 5) # largest iou is one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Train this network on ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for segmentation\n",
    "def train_seg(train_loader, test_loader, network, optimizer, epochs, scheduler):  \n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ious = []\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            ## TASK 2.12\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            loss = nn.CrossEntropyLoss()(output,label.squeeze())\n",
    "            ##########\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001        \n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            # calculate the correction \n",
    "            correct += pred.eq(label).sum().item()\n",
    "            total += label.numel()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "#             ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100))\n",
    "#         print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}; Train mean IoU: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "\n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            ious = []\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "                \n",
    "                ## TASK 2.12\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.CrossEntropyLoss()(output,label)\n",
    "                ##########\n",
    "                \n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001   \n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                total += label.numel()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100))\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}; Test mean IoU: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1134\n",
      "Average Train Loss: 1.1840; Train Acc: 76.6360\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0973\n",
      "Average Test Loss: 1.1420; Test Acc: 77.2640\n",
      "\n",
      "Average Test Loss: 1.1420; Test Acc: 77.2640; Test mean IoU: 0.6351\n",
      "-------------------------------------------\n",
      "Epoch:[02/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0977\n",
      "Average Train Loss: 1.1130; Train Acc: 80.8585\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0536\n",
      "Average Test Loss: 1.1153; Test Acc: 80.1705\n",
      "\n",
      "Average Test Loss: 1.1153; Test Acc: 80.1705; Test mean IoU: 0.6781\n",
      "-------------------------------------------\n",
      "Epoch:[03/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1047\n",
      "Average Train Loss: 1.1018; Train Acc: 81.3716\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0599\n",
      "Average Test Loss: 1.0977; Test Acc: 81.6090\n",
      "\n",
      "Average Test Loss: 1.0977; Test Acc: 81.6090; Test mean IoU: 0.6965\n",
      "-------------------------------------------\n",
      "Epoch:[04/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1003\n",
      "Average Train Loss: 1.0992; Train Acc: 81.4648\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1034\n",
      "Average Test Loss: 1.1518; Test Acc: 75.9726\n",
      "\n",
      "Average Test Loss: 1.1518; Test Acc: 75.9726; Test mean IoU: 0.6191\n",
      "-------------------------------------------\n",
      "Epoch:[05/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0902\n",
      "Average Train Loss: 1.0907; Train Acc: 81.9316\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0289\n",
      "Average Test Loss: 1.1056; Test Acc: 80.5660\n",
      "\n",
      "Average Test Loss: 1.1056; Test Acc: 80.5660; Test mean IoU: 0.6840\n",
      "-------------------------------------------\n",
      "Epoch:[06/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1060\n",
      "Average Train Loss: 1.1000; Train Acc: 81.0374\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9986\n",
      "Average Test Loss: 1.0992; Test Acc: 81.2776\n",
      "\n",
      "Average Test Loss: 1.0992; Test Acc: 81.2776; Test mean IoU: 0.6925\n",
      "-------------------------------------------\n",
      "Epoch:[07/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0779\n",
      "Average Train Loss: 1.0968; Train Acc: 81.3085\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0257\n",
      "Average Test Loss: 1.1127; Test Acc: 79.8626\n",
      "\n",
      "Average Test Loss: 1.1127; Test Acc: 79.8626; Test mean IoU: 0.6739\n",
      "-------------------------------------------\n",
      "Epoch:[08/12]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1189\n",
      "Average Train Loss: 1.0901; Train Acc: 81.8396\n",
      "\n",
      "Testing...\n",
      "Iter: [288/341] Loss: 1.1049"
     ]
    }
   ],
   "source": [
    "network = Segmentation(train_seg_dataset.num_classes, alignment=True).cuda()\n",
    "epochs = 12 # you can change the value to a small number for debugging\n",
    "# Training parameters are the same as the classiï¬cation network.\n",
    "## TASK 2.13\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)\n",
    "# # choose a lr scheduler\n",
    "# The learning rate is divided by 2 every 20 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "train_seg(train_seg_loader, test_seg_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test mIoU you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7175"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
