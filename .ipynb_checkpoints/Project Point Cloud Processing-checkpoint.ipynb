{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project, you will be asked to implement [PointNet](https://arxiv.org/abs/1612.00593) architecture and train a classification network (left) and a segmentation network (middle).\n",
    "![title](img/cls_sem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Points\n",
    "* Task 1.1 - 5\n",
    "* Task 1.2 - 5\n",
    "* Task 2.1 - 10\n",
    "* Task 2.2 - 5\n",
    "* Task 2.3 - 5\n",
    "* Task 2.4 - 5\n",
    "* Task 2.5 - 5\n",
    "* Task 2.6 - 10\n",
    "* Task 2.7 - 5\n",
    "* Task 2.8 - 10\n",
    "* Task 2.9 - 10\n",
    "* Task 2.10 - 5 \n",
    "* Task 2.11 - 5\n",
    "* Task 2.12 - 5\n",
    "* Task 2.13 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import dataset # custom dataset for ModelNet10 and ShapeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we write the point cloud as $X\\in\\mathbb{R}^{N\\times 3}$. While in programming, we use `B x 3 x N` layout, where `B` is the batch-size and `N` is the number of points in a single point cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Jitter the position of each points by a zero mean Gaussian\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we transform $X$ by $X \\leftarrow X + \\mathcal{N}(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomJitter(object):\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## hint: useful function `torch.randn` and `torch.randn_like`\n",
    "        ## TASK 1.1\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the jittered point cloud of layout `3 x N`.\n",
    "#       torch.randn mean=0, variance=1\n",
    "        distribution = torch.empty(data.shape).normal_(mean=0,std=self.sigma)\n",
    "        data = data + distribution\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test your transform here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomJitter = RandomJitter(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3520,  0.5141,  0.2169, -2.6240, -0.4214,  0.4747, -0.0183, -0.3933,\n",
       "          0.7387, -0.8958, -0.2121,  1.9848,  0.2685, -1.2039,  1.6044, -0.9679],\n",
       "        [-0.9044,  0.1868, -0.5167, -0.9403, -0.7028, -0.1019,  0.6148, -1.0564,\n",
       "         -0.8722,  1.3013, -1.1787, -0.0405,  1.0275,  0.3444, -1.9572,  0.9243],\n",
       "        [ 0.1299, -1.1597, -0.5881, -0.3834, -0.8663,  0.1068, -0.7399, -0.1256,\n",
       "         -1.3237, -1.5778, -0.0985, -0.5886,  1.2592, -0.8350, -0.1757,  2.6175]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomJitter.__call__(torch.randn(3,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rotate the object along the z-axis randomly\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we rotate all points along z-axis (up-axis) by a degree $\\theta$.\n",
    "\n",
    "\n",
    "Suppose $T$ is the transformation matrix,\n",
    "$$X\\leftarrow XT,$$\n",
    "where $$T=\\begin{bmatrix}\\cos\\theta & \\sin\\theta & 0 \\\\ -\\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[x,y,z]->[xcos\\theta-ysin\\theta, xsin\\theta-ycos\\theta, z]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomZRotate(object):\n",
    "    def __init__(self, degrees):\n",
    "        ## here `self.degrees` is a tuple (0, 360) which defines the range of degree\n",
    "        self.degrees = degrees\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## TASK 1.2\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the rotated point cloud of layout `3 x N`.\n",
    "        ##\n",
    "        ## The rotation is along z-axis, and the degree is uniformly distributed\n",
    "        ## between [0, 360]\n",
    "        ##\n",
    "        ## hint: useful function `torch.randn`ï¼Œ `torch.randn_like` and `torch.matmul`\n",
    "        ##\n",
    "        ## Notice:   \n",
    "        ## Different from its math notation `N x 3`, the input has size of `3 x N`\n",
    "        degree = torch.LongTensor(1).random_(self.degrees[0], self.degrees[1])\n",
    "#         print(degree)\n",
    "        d= 2 * np.pi * (degree/360)\n",
    "        T = torch.Tensor([[np.cos(d), np.sin(d),0],\n",
    "                         [-np.sin(d),np.cos(d),0],\n",
    "                         [0,0,1]])\n",
    "        data = torch.matmul(T,data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test your transform here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomZRotate = RandomZRotate((0,360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7704,  1.1422, -0.6398,  0.4064, -1.0655, -1.3393,  0.5662,  0.6398,\n",
       "         -0.1455,  1.3356, -1.0211, -1.1103,  0.8489,  0.1762,  0.7887, -0.5296],\n",
       "        [-1.2052,  0.0097,  0.2326, -0.9892,  0.0609,  1.3887,  1.2696,  0.2261,\n",
       "         -2.5262,  0.9990,  0.2980,  0.4682,  0.8535,  0.4311, -0.5221, -1.3735],\n",
       "        [ 1.3465,  1.0376,  0.5592,  1.5081, -0.5739, -0.3559,  0.8704,  0.0045,\n",
       "          0.2031, -0.1129, -0.6363, -2.5462, -0.4523,  0.2583, -0.3420, -0.0498]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomZRotate.__call__(torch.randn(3,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load dataset ModelNet10 for Point Cloud Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelNet10\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and label of shape `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may taske some time to download and pre-process the dataset.\n",
    "train_transform = Compose([RandomZRotate((0, 360)), RandomJitter(0.02)])\n",
    "train_cls_dataset = dataset.ModelNet(root='./ModelNet10', transform=train_transform, train=True)\n",
    "test_cls_dataset = dataset.ModelNet(root='./ModelNet10', train=False)\n",
    "train_cls_loader = data.DataLoader(\n",
    "    train_cls_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_cls_loader = data.DataLoader(\n",
    "    test_cls_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_cls_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShapeNet\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and target of shape `B x N`.\n",
    "\n",
    "Here is the list of categories:\n",
    "['Airplane', 'Bag', 'Cap', 'Car', 'Chair', 'Earphone', 'Guitar', 'Knife', 'Lamp', 'Laptop', 'Motorbike', 'Mug', 'Pistol', 'Rocket', 'Skateboard', 'Table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">classification for every point</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here as an example, we choose the cateogry 'Airplane'\n",
    "category = 'Airplane'\n",
    "train_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=True)\n",
    "test_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=False)\n",
    "train_seg_loader = data.DataLoader(\n",
    "    train_seg_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_seg_loader = data.DataLoader(\n",
    "    test_seg_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_seg_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PointNet Architecture (Read Section 4.2 and Appendix C)\n",
    "In this section, you will be asked to implement classification and segmentation step by step.\n",
    "![pointnet](img/pointnet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Joint Alignment Network \n",
    "This mini-network takes as input matrix of size $N \\times K$, and outputs a transformation matrix of size $K \\times K$. \n",
    "\n",
    "In programming, the input size of this module is `B x K x N` and output size is `B x K x K`.\n",
    "\n",
    "For the shared MLP, use structure like this `(FC(64), BN, ReLU, FC(128), BN, ReLU, FC(1024), BN, ReLU)`.\n",
    "\n",
    "For the MLP after global max pooling, use structure like this `(FC(512), BN, ReLU, FC(256), BN, ReLU, FC(K*K)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super(Transformation, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        ## TASK 2.1\n",
    "        \n",
    "        ## define your network layers here\n",
    "        ## shared mlp\n",
    "        ## input size: B x K x N\n",
    "        ## output size: B x 1024 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here. Why?\n",
    "        # depthwise groups=64? one filter will work on all of channels seperately, thus the number of output channels will be k*input_channels\n",
    "        self.share_mlp = nn.Sequential(nn.Conv1d(k, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 64*2,1, stride=1),\n",
    "                                      nn.BatchNorm1d(64*2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64*2, 64*16, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64*16),\n",
    "                                      nn.ReLU())\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x (K*K)\n",
    "        self.mlp = nn.Sequential(nn.Linear(64*16, 64*8), \n",
    "                                 nn.BatchNorm1d(64*8),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64*8, 64*4),\n",
    "                                 # batch size should be larger than 1, otherwise there will have an error\n",
    "                                 nn.BatchNorm1d(64*4),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64*4, k**2))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, K, N = x.shape # batch-size, dim, number of points\n",
    "        ## TASK 2.1\n",
    "        self.k = K\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.share_mlp(x)\n",
    "#         ## global max pooling\n",
    "#         # input - B x 1024 x N\n",
    "#         # output - B x 1024\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "#         ## mlp\n",
    "#         # input - B x 1024\n",
    "#         # output - B x (K*K)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        ## reshape the transformation matrix to B x K x K\n",
    "        identity = torch.eye(self.k, device=x.device)\n",
    "        x = x.view(B, self.k, self.k) + identity[None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = Transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation(torch.randn(5,3,8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regularization Loss\n",
    "$$L_{reg}=\\|I-TT^\\intercal\\|^2_F$$\n",
    "\n",
    "The output of `Transformation` network is of size `B x K x K`. The module `OrthoLoss` has no trainable parameters, only computes this norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrthoLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## hint: useful function `torch.bmm` and `torch.matmul`\n",
    "#         Performs a batch matrix-matrix product of matrices\n",
    "        ## TASK 2.2\n",
    "        ## compute the matrix product\n",
    "        prod = torch.bmm(x,torch.transpose(x, 1, 2))\n",
    "        norm = torch.norm(prod - torch.eye(x.shape[1], device=x.device)[None], dim=(1,2))\n",
    "        return norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthoLoss = OrthoLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0467623999999995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.7844)**2+(-1.3648)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7844, -1.3648],\n",
      "         [-0.6096, -1.7659]],\n",
      "\n",
      "        [[-1.1674, -0.6116],\n",
      "         [-0.6308, -0.4505]]])\n",
      "tensor([[[-1.7844, -0.6096],\n",
      "         [-1.3648, -1.7659]],\n",
      "\n",
      "        [[-1.1674, -0.6308],\n",
      "         [-0.6116, -0.4505]]])\n",
      "tensor([[[5.0470, 3.4978],\n",
      "         [3.4978, 3.4899]],\n",
      "\n",
      "        [[1.7370, 1.0120],\n",
      "         [1.0120, 0.6009]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.2588)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthoLoss(torch.randn(2,2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Network\n",
    "In this subsection, you will be asked to implement the feature network (the top branch).\n",
    "\n",
    "Local features are a matrix of size `B x 64 x N`, which will be used in the segmentation task.\n",
    "\n",
    "Global features are a matrix of size `B x 1024`, which will be used in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self, alignment=False):\n",
    "        super(Feature, self).__init__()\n",
    "        \n",
    "        self.alignment = alignment\n",
    "        \n",
    "        ## `input_transform` calculates the input transform matrix of size `3 x 3`\n",
    "        if self.alignment:\n",
    "            self.input_transform = Transformation(3)\n",
    "        \n",
    "        ## TASK 2.3\n",
    "        ## define your network layers here\n",
    "        ## local feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 3 x N\n",
    "        ## output size: B x 64 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here.\n",
    "        self.local_feature = nn.Sequential(nn.Conv1d(3, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU())\n",
    "        ## `feature_transform` calculates the feature transform matrix of size `64 x 64`\n",
    "        if self.alignment:\n",
    "            self.feature_transform = Transformation(64)\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## define your network layers here\n",
    "        ## global feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 64 x N\n",
    "        ## output size: B x 1024 x N\n",
    "        self.global_feature = nn.Sequential(\n",
    "                              nn.Conv1d(64, 64*2,1, stride=1),\n",
    "                              nn.BatchNorm1d(64*2),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(64*2, 64*16, 1, stride=1),\n",
    "                              nn.BatchNorm1d(64*16),\n",
    "                              nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,K,N = x.shape\n",
    "        ## apply the input transform\n",
    "        if self.alignment:\n",
    "            transform = self.input_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the input transform\n",
    "            x = torch.bmm(transform,x)\n",
    "    \n",
    "\n",
    "        ## TASK 2.3\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 64 x N\n",
    "        x = self.local_feature(x)\n",
    "        \n",
    "        if self.alignment:\n",
    "            transform = self.feature_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the feature transform\n",
    "            x = torch.bmm(transform,x)\n",
    "        else:\n",
    "            ## do not modify this line\n",
    "            transform = None\n",
    "        \n",
    "        local_feature = x\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 64 x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.global_feature(x)\n",
    "        \n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## global max pooling\n",
    "        # input - B x 1024 x N\n",
    "        # output - B x 1024\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.view(-1,1024)\n",
    "        global_feature = x\n",
    "        ## summary:\n",
    "        ## global_feature: B x 1024\n",
    "        ## local_feature: B x 64 x N\n",
    "        ## transform: B x K x K\n",
    "        return global_feature, local_feature, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = Feature(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = feature(torch.randn(3,3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transform: B x K x K the size of transform??\n",
    "result[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification Network\n",
    "In this network, you will use the global features generated by the `Feature` network defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Classification, self).__init__()\n",
    "                \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "\n",
    "        ## TASK 2.6\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x num_classes=10\n",
    "        self.mlp = nn.Sequential(nn.Linear(64*16, 64*8),\n",
    "                         nn.BatchNorm1d(64*8),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64*8, 64*4),\n",
    "                         # batch size should be larger than 1, otherwise there will have an error\n",
    "                         nn.BatchNorm1d(64*4),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64*4, 64*2),\n",
    "                         nn.Dropout(0.3),\n",
    "                         nn.BatchNorm1d(64*2),\n",
    "                         nn.ReLU(),\n",
    "#                          nn.Linear(64*2, 64),\n",
    "#                          nn.Dropout(0.3),\n",
    "#                          nn.BatchNorm1d(64),\n",
    "#                          nn.ReLU(),\n",
    "                         nn.Linear(64*2, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the global feature matrix\n",
    "        # the size of global_feature: B x 1024\n",
    "        # here we don't use local feature matrix\n",
    "        x, _, trans = self.feature(x)\n",
    "        \n",
    "        ## TASK 2.6\n",
    "        ## forward of mlp\n",
    "        # input - B x 1024\n",
    "        # output - B x num_classes        \n",
    "        x = self.mlp(x)\n",
    "        ## x: B x num_classes\n",
    "        ## trans: B x K x K\n",
    "        # add a sigmoid\n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes=10\n",
    "classification = Classification(10)\n",
    "result = classification(torch.randn(3,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Train this network on ModelNet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "def train_cls(train_loader, test_loader, network, optimizer, epochs, scheduler):\n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            \n",
    "            ## TASK 2.7\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            loss = loss = nn.CrossEntropyLoss()(output,label)\n",
    "            ##########\n",
    "            \n",
    "            ## regularizer\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001\n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(label).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/len(train_loader.dataset) * 100))\n",
    "        \n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "    \n",
    "                ## TASK 2.7\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.CrossEntropyLoss()(output,label)\n",
    "                ##########\n",
    "\n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001\n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/len(test_loader.dataset) * 100))\n",
    "        print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1221\n",
      "Average Train Loss: 1.2781; Train Acc: 63.7434\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0689\n",
      "Average Test Loss: 1.1996; Test Acc: 60.5727\n",
      "-------------------------------------------\n",
      "Epoch:[02/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5967\n",
      "Average Train Loss: 0.8436; Train Acc: 75.2443\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1085\n",
      "Average Test Loss: 0.8869; Test Acc: 71.5859\n",
      "-------------------------------------------\n",
      "Epoch:[03/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9886\n",
      "Average Train Loss: 0.6806; Train Acc: 79.3034\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1933\n",
      "Average Test Loss: 0.8565; Test Acc: 71.3656\n",
      "-------------------------------------------\n",
      "Epoch:[04/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5077\n",
      "Average Train Loss: 0.5964; Train Acc: 80.9321\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0869\n",
      "Average Test Loss: 0.6619; Test Acc: 75.8811\n",
      "-------------------------------------------\n",
      "Epoch:[05/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6200\n",
      "Average Train Loss: 0.5120; Train Acc: 83.9389\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0582\n",
      "Average Test Loss: 0.5459; Test Acc: 82.2687\n",
      "-------------------------------------------\n",
      "Epoch:[06/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4226\n",
      "Average Train Loss: 0.4306; Train Acc: 86.3693\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0681\n",
      "Average Test Loss: 0.6126; Test Acc: 78.8546\n",
      "-------------------------------------------\n",
      "Epoch:[07/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.5736\n",
      "Average Train Loss: 0.4223; Train Acc: 86.6700\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01146\n",
      "Average Test Loss: 0.4865; Test Acc: 80.8370\n",
      "-------------------------------------------\n",
      "Epoch:[08/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.6405\n",
      "Average Train Loss: 0.4104; Train Acc: 87.1962\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0430\n",
      "Average Test Loss: 0.6667; Test Acc: 76.5419\n",
      "-------------------------------------------\n",
      "Epoch:[09/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.3693\n",
      "Average Train Loss: 0.3789; Train Acc: 88.2987\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03685\n",
      "Average Test Loss: 0.4497; Test Acc: 85.9031\n",
      "-------------------------------------------\n",
      "Epoch:[10/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2454\n",
      "Average Train Loss: 0.3748; Train Acc: 88.0732\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0812\n",
      "Average Test Loss: 0.3955; Test Acc: 86.2335\n",
      "-------------------------------------------\n",
      "Epoch:[11/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2447\n",
      "Average Train Loss: 0.3162; Train Acc: 90.3032\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03723\n",
      "Average Test Loss: 0.5153; Test Acc: 84.2511\n",
      "-------------------------------------------\n",
      "Epoch:[12/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.8742\n",
      "Average Train Loss: 0.3387; Train Acc: 89.1255\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0088\n",
      "Average Test Loss: 0.3435; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[13/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5929\n",
      "Average Train Loss: 0.2855; Train Acc: 91.3806\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0104\n",
      "Average Test Loss: 0.3157; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[14/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1845\n",
      "Average Train Loss: 0.2941; Train Acc: 90.8544\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0597\n",
      "Average Test Loss: 0.4026; Test Acc: 87.1145\n",
      "-------------------------------------------\n",
      "Epoch:[15/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6049\n",
      "Average Train Loss: 0.2757; Train Acc: 91.1300\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00602\n",
      "Average Test Loss: 0.5115; Test Acc: 82.9295\n",
      "-------------------------------------------\n",
      "Epoch:[16/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1009\n",
      "Average Train Loss: 0.2818; Train Acc: 91.1300\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0095\n",
      "Average Test Loss: 0.3463; Test Acc: 89.8678\n",
      "-------------------------------------------\n",
      "Epoch:[17/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0336\n",
      "Average Train Loss: 0.2732; Train Acc: 91.2303\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00854\n",
      "Average Test Loss: 0.3464; Test Acc: 88.5463\n",
      "-------------------------------------------\n",
      "Epoch:[18/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3386\n",
      "Average Train Loss: 0.2598; Train Acc: 92.1824\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.02769\n",
      "Average Test Loss: 0.3996; Test Acc: 87.1145\n",
      "-------------------------------------------\n",
      "Epoch:[19/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9682\n",
      "Average Train Loss: 0.2414; Train Acc: 92.6084\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0066\n",
      "Average Test Loss: 0.3062; Test Acc: 90.6388\n",
      "-------------------------------------------\n",
      "Epoch:[20/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3863\n",
      "Average Train Loss: 0.2395; Train Acc: 92.3578\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.11828\n",
      "Average Test Loss: 0.4160; Test Acc: 83.3700\n",
      "-------------------------------------------\n",
      "Epoch:[21/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1149\n",
      "Average Train Loss: 0.1886; Train Acc: 94.0366\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00430\n",
      "Average Test Loss: 0.2913; Test Acc: 91.1894\n",
      "-------------------------------------------\n",
      "Epoch:[22/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1823\n",
      "Average Train Loss: 0.1619; Train Acc: 94.6630\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00461\n",
      "Average Test Loss: 0.2858; Test Acc: 89.5374\n",
      "-------------------------------------------\n",
      "Epoch:[23/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0198\n",
      "Average Train Loss: 0.1852; Train Acc: 94.3874\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00641\n",
      "Average Test Loss: 0.3064; Test Acc: 90.4185\n",
      "-------------------------------------------\n",
      "Epoch:[24/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6181\n",
      "Average Train Loss: 0.1814; Train Acc: 94.1368\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00354\n",
      "Average Test Loss: 0.3509; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[25/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0347\n",
      "Average Train Loss: 0.1665; Train Acc: 94.7632\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00655\n",
      "Average Test Loss: 0.3151; Test Acc: 87.9956\n",
      "-------------------------------------------\n",
      "Epoch:[26/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0133\n",
      "Average Train Loss: 0.1545; Train Acc: 95.0639\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00343\n",
      "Average Test Loss: 0.3248; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[27/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5220\n",
      "Average Train Loss: 0.1514; Train Acc: 95.0388\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00317\n",
      "Average Test Loss: 0.2822; Test Acc: 90.5286\n",
      "-------------------------------------------\n",
      "Epoch:[28/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1483\n",
      "Average Train Loss: 0.1448; Train Acc: 95.6151\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00429\n",
      "Average Test Loss: 0.2978; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[29/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2130\n",
      "Average Train Loss: 0.1505; Train Acc: 94.6630\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0028\n",
      "Average Test Loss: 0.2782; Test Acc: 91.2996\n",
      "-------------------------------------------\n",
      "Epoch:[30/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.4608\n",
      "Average Train Loss: 0.1515; Train Acc: 95.3145\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00331\n",
      "Average Test Loss: 0.2580; Test Acc: 91.6300\n",
      "-------------------------------------------\n",
      "Epoch:[31/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6616\n",
      "Average Train Loss: 0.1393; Train Acc: 95.6151\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00258\n",
      "Average Test Loss: 0.4265; Test Acc: 85.3524\n",
      "-------------------------------------------\n",
      "Epoch:[32/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0254\n",
      "Average Train Loss: 0.1338; Train Acc: 95.6151\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00424\n",
      "Average Test Loss: 0.3688; Test Acc: 87.8855\n",
      "-------------------------------------------\n",
      "Epoch:[33/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0517\n",
      "Average Train Loss: 0.1349; Train Acc: 95.7404\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00185\n",
      "Average Test Loss: 0.2684; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[34/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0202\n",
      "Average Train Loss: 0.1223; Train Acc: 96.0411\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00279\n",
      "Average Test Loss: 0.3181; Test Acc: 88.3260\n",
      "-------------------------------------------\n",
      "Epoch:[35/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0171\n",
      "Average Train Loss: 0.1135; Train Acc: 96.0661\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00217\n",
      "Average Test Loss: 0.2974; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[36/60]\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [250/250] Loss: 0.0288\n",
      "Average Train Loss: 0.1202; Train Acc: 95.8406\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00634\n",
      "Average Test Loss: 0.3058; Test Acc: 89.0969\n",
      "-------------------------------------------\n",
      "Epoch:[37/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1154\n",
      "Average Train Loss: 0.1342; Train Acc: 95.3395\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00203\n",
      "Average Test Loss: 0.2889; Test Acc: 90.8590\n",
      "-------------------------------------------\n",
      "Epoch:[38/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.0603\n",
      "Average Train Loss: 0.1491; Train Acc: 95.4147\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0057\n",
      "Average Test Loss: 0.3011; Test Acc: 90.5286\n",
      "-------------------------------------------\n",
      "Epoch:[39/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0994\n",
      "Average Train Loss: 0.1271; Train Acc: 95.3145\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00132\n",
      "Average Test Loss: 0.3127; Test Acc: 90.7489\n",
      "-------------------------------------------\n",
      "Epoch:[40/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0430\n",
      "Average Train Loss: 0.1405; Train Acc: 95.5650\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00355\n",
      "Average Test Loss: 0.3763; Test Acc: 87.9956\n",
      "-------------------------------------------\n",
      "Epoch:[41/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.8104\n",
      "Average Train Loss: 0.1167; Train Acc: 96.4169\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00337\n",
      "Average Test Loss: 0.2554; Test Acc: 91.0793\n",
      "-------------------------------------------\n",
      "Epoch:[42/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4666\n",
      "Average Train Loss: 0.1021; Train Acc: 96.7427\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00173\n",
      "Average Test Loss: 0.2604; Test Acc: 90.1982\n",
      "-------------------------------------------\n",
      "Epoch:[43/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2091\n",
      "Average Train Loss: 0.0934; Train Acc: 96.5673\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00222\n",
      "Average Test Loss: 0.2875; Test Acc: 90.1982\n",
      "-------------------------------------------\n",
      "Epoch:[44/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0584\n",
      "Average Train Loss: 0.0889; Train Acc: 97.1436\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00360\n",
      "Average Test Loss: 0.2850; Test Acc: 89.2070\n",
      "-------------------------------------------\n",
      "Epoch:[45/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7305\n",
      "Average Train Loss: 0.1001; Train Acc: 96.4420\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00150\n",
      "Average Test Loss: 0.2876; Test Acc: 89.3172\n",
      "-------------------------------------------\n",
      "Epoch:[46/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1176\n",
      "Average Train Loss: 0.0823; Train Acc: 97.0935\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00577\n",
      "Average Test Loss: 0.2847; Test Acc: 90.7489\n",
      "-------------------------------------------\n",
      "Epoch:[47/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.4432\n",
      "Average Train Loss: 0.0858; Train Acc: 97.1937\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00163\n",
      "Average Test Loss: 0.2837; Test Acc: 90.8590\n",
      "-------------------------------------------\n",
      "Epoch:[48/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.4244\n",
      "Average Train Loss: 0.0913; Train Acc: 96.7677\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00250\n",
      "Average Test Loss: 0.2839; Test Acc: 89.8678\n",
      "-------------------------------------------\n",
      "Epoch:[49/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0244\n",
      "Average Train Loss: 0.0796; Train Acc: 97.0935\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00269\n",
      "Average Test Loss: 0.2879; Test Acc: 90.6388\n",
      "-------------------------------------------\n",
      "Epoch:[50/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3351\n",
      "Average Train Loss: 0.0799; Train Acc: 97.2187\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0013\n",
      "Average Test Loss: 0.2829; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[51/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3966\n",
      "Average Train Loss: 0.0682; Train Acc: 97.6447\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00170\n",
      "Average Test Loss: 0.2904; Test Acc: 89.3172\n",
      "-------------------------------------------\n",
      "Epoch:[52/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1474\n",
      "Average Train Loss: 0.0760; Train Acc: 97.3691\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00158\n",
      "Average Test Loss: 0.2781; Test Acc: 90.1982\n",
      "-------------------------------------------\n",
      "Epoch:[53/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1841\n",
      "Average Train Loss: 0.0699; Train Acc: 97.5445\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00188\n",
      "Average Test Loss: 0.2745; Test Acc: 91.4097\n",
      "-------------------------------------------\n",
      "Epoch:[54/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0130\n",
      "Average Train Loss: 0.0671; Train Acc: 97.5445\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00316\n",
      "Average Test Loss: 0.2663; Test Acc: 91.0793\n",
      "-------------------------------------------\n",
      "Epoch:[55/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0092\n",
      "Average Train Loss: 0.0757; Train Acc: 97.4192\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00141\n",
      "Average Test Loss: 0.2800; Test Acc: 89.2070\n",
      "-------------------------------------------\n",
      "Epoch:[56/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0167\n",
      "Average Train Loss: 0.0748; Train Acc: 97.4693\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00098\n",
      "Average Test Loss: 0.2898; Test Acc: 90.7489\n",
      "-------------------------------------------\n",
      "Epoch:[57/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4633\n",
      "Average Train Loss: 0.0736; Train Acc: 97.3440\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00190\n",
      "Average Test Loss: 0.3279; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[58/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0088\n",
      "Average Train Loss: 0.0793; Train Acc: 96.9932\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00166\n",
      "Average Test Loss: 0.2928; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[59/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0117\n",
      "Average Train Loss: 0.0667; Train Acc: 98.0205\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00124\n",
      "Average Test Loss: 0.2958; Test Acc: 89.9780\n",
      "-------------------------------------------\n",
      "Epoch:[60/60]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4534\n",
      "Average Train Loss: 0.0672; Train Acc: 97.5946\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00218\n",
      "Average Test Loss: 0.3070; Test Acc: 90.3084\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "network = Classification(10, alignment=True).cuda()\n",
    "epochs = 60 # you can change the value to a small number for debugging\n",
    "\n",
    "## TASK 2.8\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "# lr=0.001 is the default value\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)\n",
    "# # choose a lr scheduler\n",
    "# The learning rate is divided by 2 every 20 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "# start training\n",
    "train_cls(train_cls_loader, test_cls_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout can be considered to added to outcome the overfitting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Best test accuracy</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>2019-11-21</th>\n",
    "    <th>91.8502</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>2019-11-29</th>\n",
    "    <th>98.0205</th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Segmentation Network\n",
    "In this network, you will use the global features and local features generated by the `Feature` network defined above.\n",
    "\n",
    "The global feature matrix is of size `B x 1024` and the local feature matrix is of size `B x 64 x N`.\n",
    "\n",
    "They need to be stacked together to a new matrix of size `B x 1088 x n` (How?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Segmentation, self).__init__()\n",
    "               \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "\n",
    "        ## TASK 2.9\n",
    "        ## shared mlp\n",
    "        ## input size: B x 1088 x N\n",
    "        ## output size: B x num_classes x N\n",
    "        self.shared_mlp = nn.Sequential(nn.Conv1d(1088, 512, 1, stride=1),\n",
    "                              nn.BatchNorm1d(512),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(512, 256, 1, stride=1),\n",
    "                              nn.BatchNorm1d(256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(256, 128, 1, stride=1), \n",
    "                              nn.BatchNorm1d(128), \n",
    "                              nn.ReLU(), \n",
    "                              nn.Conv1d(128, num_classes, 1, stride=1),\n",
    "                              nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        g, l, trans = self.feature(x)\n",
    "        _,_,N = l.shape\n",
    "        ## TASK 2.10\n",
    "        # concat global features and local features to a single matrix\n",
    "        # g - B x 1024, global features\n",
    "        # l - B x 64 x N, local features\n",
    "        # x - B x 1088 x N, concatenated features\n",
    "        g = g.view(-1,1024,1)\n",
    "        g = torch.repeat_interleave(g, repeats=N, dim=2)\n",
    "        x = torch.cat((l, g),dim=1)\n",
    "        ## TASK 2.9\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 1088 x N\n",
    "        # output - B x num_classes x N  \n",
    "        x = self.shared_mlp(x)\n",
    "        \n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generate data and test this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 10])\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 1024, 10])\n",
      "torch.Size([2, 1088, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3085, 0.3580, 0.2055, 0.3077, 0.1827, 0.2007, 0.2444, 0.1436,\n",
       "          0.2685, 0.2685],\n",
       "         [0.1180, 0.0993, 0.1350, 0.1060, 0.1260, 0.1242, 0.2211, 0.2402,\n",
       "          0.1519, 0.1605],\n",
       "         [0.1610, 0.1450, 0.1800, 0.1362, 0.1675, 0.2704, 0.0992, 0.2245,\n",
       "          0.1378, 0.1410],\n",
       "         [0.1881, 0.2316, 0.1807, 0.1233, 0.2530, 0.1836, 0.1218, 0.1965,\n",
       "          0.1605, 0.1336],\n",
       "         [0.2244, 0.1660, 0.2988, 0.3267, 0.2707, 0.2212, 0.3136, 0.1951,\n",
       "          0.2813, 0.2965]],\n",
       "\n",
       "        [[0.1676, 0.2040, 0.1574, 0.2405, 0.2126, 0.1016, 0.1611, 0.2288,\n",
       "          0.1782, 0.1799],\n",
       "         [0.1393, 0.1075, 0.1346, 0.1307, 0.0588, 0.0962, 0.1500, 0.1068,\n",
       "          0.1625, 0.1115],\n",
       "         [0.1833, 0.2392, 0.2997, 0.2236, 0.1437, 0.2246, 0.2608, 0.1987,\n",
       "          0.1709, 0.1799],\n",
       "         [0.1369, 0.2200, 0.0913, 0.1767, 0.2773, 0.3039, 0.1483, 0.1531,\n",
       "          0.1404, 0.2840],\n",
       "         [0.3730, 0.2293, 0.3171, 0.2286, 0.3076, 0.2737, 0.2799, 0.3125,\n",
       "          0.3480, 0.2447]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation = Segmentation(5)\n",
    "# B x 3 x N \n",
    "segmentation(torch.randn(2,3,10))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Calculating Intersection over Union (IoU) \n",
    "For 2D image, the IoU is calculated as follows,\n",
    "![iou](img/iou.png)\n",
    "\n",
    "How is it used in the literature of point clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 2.11\n",
    "# implement the helper functions to calculate the IoU\n",
    "def get_i_and_u(pred, target, num_classes):\n",
    "    \"\"\"Calculate intersection and union between pred and target.\n",
    "    \n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return i, u\n",
    "    i -- B x N binary matrix, intersection, i[b, n] equals 1 if and only if it is a true-positive.\n",
    "    u -- B x N binary matrix, union, u[b, n] equals 0 if and only if it is a true-negative\n",
    "    \"\"\"\n",
    "    ## TASK 2.11\n",
    "    ## calculate i and u here\n",
    "    ## hint: useful function `F.one_hot`    \n",
    "    ## hint: use element-wise logical tensor operation (`&` and `|`)\n",
    "    target_onehot = F.one_hot(target, num_classes=num_classes)\n",
    "    pre_onehot = F.one_hot(pred, num_classes=num_classes)\n",
    "    \n",
    "    i = torch.sum((target_onehot & pre_onehot).type(torch.float64), dim=1)\n",
    "    u = torch.sum((target_onehot | pre_onehot).type(torch.float64), dim=1)\n",
    "\n",
    "    return i, u\n",
    "\n",
    "def get_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculate IoU\n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return iou\n",
    "    iou -- B matrix, iou[b] is the IoU of b-th point cloud in this batch\n",
    "    \"\"\"\n",
    "    \n",
    "    ## use the helper function `i_and_u` defined above\n",
    "    i, u = get_i_and_u(pred, target, num_classes)\n",
    "    \n",
    "    ## TASK 2.11\n",
    "    ## calculate iou\n",
    "    iou = torch.sum(i,dim=1) / torch.sum(u,dim=1)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[2,3],[1,2]])\n",
    "b = torch.LongTensor([[2,3],[1,2]])\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = F.one_hot(a, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ = F.one_hot(b, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0]]])\n",
      "tensor([[[0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "print(a_) # target\n",
    "print(b_) # predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0., 0.]], dtype=torch.float64),\n",
       " tensor([[0., 0., 1., 1., 0.],\n",
       "         [0., 1., 1., 0., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_i_and_u(a, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_iou(a, b, 5) # largest iou is one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Train this network on ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for segmentation\n",
    "def train_seg(train_loader, test_loader, network, optimizer, epochs, scheduler):  \n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ious = []\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            ## TASK 2.12\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            loss = nn.CrossEntropyLoss()(output,label.squeeze())\n",
    "            ##########\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001        \n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            # calculate the correction \n",
    "            correct += pred.eq(label).sum().item()\n",
    "            total += label.numel()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "#             ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100))\n",
    "#         print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}; Train mean IoU: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "\n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            ious = []\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "                \n",
    "                ## TASK 2.12\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.CrossEntropyLoss()(output,label)\n",
    "                ##########\n",
    "                \n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001   \n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                total += label.numel()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100))\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}; Test mean IoU: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.1215\n",
      "Average Train Loss: 1.1807; Train Acc: 76.7543\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0999\n",
      "Average Test Loss: 1.1444; Test Acc: 78.8032\n",
      "\n",
      "Average Test Loss: 1.1444; Test Acc: 78.8032; Test mean IoU: 0.6596\n",
      "-------------------------------------------\n",
      "Epoch:[02/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0819\n",
      "Average Train Loss: 1.1005; Train Acc: 81.6057\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0674\n",
      "Average Test Loss: 1.1211; Test Acc: 79.6234\n",
      "\n",
      "Average Test Loss: 1.1211; Test Acc: 79.6234; Test mean IoU: 0.6679\n",
      "-------------------------------------------\n",
      "Epoch:[03/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0950\n",
      "Average Train Loss: 1.0958; Train Acc: 81.8556\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0139\n",
      "Average Test Loss: 1.0894; Test Acc: 82.2180\n",
      "\n",
      "Average Test Loss: 1.0894; Test Acc: 82.2180; Test mean IoU: 0.7046\n",
      "-------------------------------------------\n",
      "Epoch:[04/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0471\n",
      "Average Train Loss: 1.0860; Train Acc: 82.4436\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0191\n",
      "Average Test Loss: 1.0870; Test Acc: 81.9831\n",
      "\n",
      "Average Test Loss: 1.0870; Test Acc: 81.9831; Test mean IoU: 0.7010\n",
      "-------------------------------------------\n",
      "Epoch:[05/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0732\n",
      "Average Train Loss: 1.0831; Train Acc: 82.7144\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9992\n",
      "Average Test Loss: 1.0823; Test Acc: 82.9671\n",
      "\n",
      "Average Test Loss: 1.0823; Test Acc: 82.9671; Test mean IoU: 0.7153\n",
      "-------------------------------------------\n",
      "Epoch:[06/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0927\n",
      "Average Train Loss: 1.0854; Train Acc: 82.4528\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1308\n",
      "Average Test Loss: 1.1775; Test Acc: 72.8331\n",
      "\n",
      "Average Test Loss: 1.1775; Test Acc: 72.8331; Test mean IoU: 0.5785\n",
      "-------------------------------------------\n",
      "Epoch:[07/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0751\n",
      "Average Train Loss: 1.0816; Train Acc: 82.7145\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9899\n",
      "Average Test Loss: 1.0706; Test Acc: 83.5303\n",
      "\n",
      "Average Test Loss: 1.0706; Test Acc: 83.5303; Test mean IoU: 0.7232\n",
      "-------------------------------------------\n",
      "Epoch:[08/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0721\n",
      "Average Train Loss: 1.0796; Train Acc: 82.7987\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0187\n",
      "Average Test Loss: 1.0771; Test Acc: 82.8441\n",
      "\n",
      "Average Test Loss: 1.0771; Test Acc: 82.8441; Test mean IoU: 0.7130\n",
      "-------------------------------------------\n",
      "Epoch:[09/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0509\n",
      "Average Train Loss: 1.0764; Train Acc: 83.0806\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0145\n",
      "Average Test Loss: 1.0687; Test Acc: 83.6137\n",
      "\n",
      "Average Test Loss: 1.0687; Test Acc: 83.6137; Test mean IoU: 0.7246\n",
      "-------------------------------------------\n",
      "Epoch:[10/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0690\n",
      "Average Train Loss: 1.0760; Train Acc: 83.0439\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9842\n",
      "Average Test Loss: 1.1545; Test Acc: 77.4838\n",
      "\n",
      "Average Test Loss: 1.1545; Test Acc: 77.4838; Test mean IoU: 0.6423\n",
      "-------------------------------------------\n",
      "Epoch:[11/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0896\n",
      "Average Train Loss: 1.0840; Train Acc: 82.3888\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1270\n",
      "Average Test Loss: 1.0801; Test Acc: 82.5608\n",
      "\n",
      "Average Test Loss: 1.0801; Test Acc: 82.5608; Test mean IoU: 0.7088\n",
      "-------------------------------------------\n",
      "Epoch:[12/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0852\n",
      "Average Train Loss: 1.0771; Train Acc: 82.9121\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0518\n",
      "Average Test Loss: 1.0820; Test Acc: 82.5377\n",
      "\n",
      "Average Test Loss: 1.0820; Test Acc: 82.5377; Test mean IoU: 0.7083\n",
      "-------------------------------------------\n",
      "Epoch:[13/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0466\n",
      "Average Train Loss: 1.0773; Train Acc: 82.8823\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0220\n",
      "Average Test Loss: 1.1105; Test Acc: 79.3238\n",
      "\n",
      "Average Test Loss: 1.1105; Test Acc: 79.3238; Test mean IoU: 0.6626\n",
      "-------------------------------------------\n",
      "Epoch:[14/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0588\n",
      "Average Train Loss: 1.0685; Train Acc: 83.7337\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9744\n",
      "Average Test Loss: 1.0648; Test Acc: 83.9649\n",
      "\n",
      "Average Test Loss: 1.0648; Test Acc: 83.9649; Test mean IoU: 0.7309\n",
      "-------------------------------------------\n",
      "Epoch:[15/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0506\n",
      "Average Train Loss: 1.0644; Train Acc: 84.0804\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0428\n",
      "Average Test Loss: 1.0906; Test Acc: 81.2912\n",
      "\n",
      "Average Test Loss: 1.0906; Test Acc: 81.2912; Test mean IoU: 0.6906\n",
      "-------------------------------------------\n",
      "Epoch:[16/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0358\n",
      "Average Train Loss: 1.0636; Train Acc: 84.1489\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9873\n",
      "Average Test Loss: 1.0610; Test Acc: 84.3189\n",
      "\n",
      "Average Test Loss: 1.0610; Test Acc: 84.3189; Test mean IoU: 0.7364\n",
      "-------------------------------------------\n",
      "Epoch:[17/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0532\n",
      "Average Train Loss: 1.0616; Train Acc: 84.3295\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9874\n",
      "Average Test Loss: 1.0572; Test Acc: 84.6819\n",
      "\n",
      "Average Test Loss: 1.0572; Test Acc: 84.6819; Test mean IoU: 0.7416\n",
      "-------------------------------------------\n",
      "Epoch:[18/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0442\n",
      "Average Train Loss: 1.0567; Train Acc: 84.8301\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.4707\n",
      "Average Test Loss: 1.3293; Test Acc: 56.5594\n",
      "\n",
      "Average Test Loss: 1.3293; Test Acc: 56.5594; Test mean IoU: 0.4031\n",
      "-------------------------------------------\n",
      "Epoch:[19/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0105\n",
      "Average Train Loss: 1.0173; Train Acc: 88.8444\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0647\n",
      "Average Test Loss: 1.1443; Test Acc: 75.7923\n",
      "\n",
      "Average Test Loss: 1.1443; Test Acc: 75.7923; Test mean IoU: 0.6169\n",
      "-------------------------------------------\n",
      "Epoch:[20/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0403\n",
      "Average Train Loss: 1.0070; Train Acc: 89.8238\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0192\n",
      "Average Test Loss: 1.1555; Test Acc: 74.5653\n",
      "\n",
      "Average Test Loss: 1.1555; Test Acc: 74.5653; Test mean IoU: 0.6018\n",
      "-------------------------------------------\n",
      "Epoch:[21/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0166\n",
      "Average Train Loss: 0.9993; Train Acc: 90.5634\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9861\n",
      "Average Test Loss: 1.0337; Test Acc: 87.0684\n",
      "\n",
      "Average Test Loss: 1.0337; Test Acc: 87.0684; Test mean IoU: 0.7773\n",
      "-------------------------------------------\n",
      "Epoch:[22/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9797\n",
      "Average Train Loss: 0.9952; Train Acc: 90.9488\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9896\n",
      "Average Test Loss: 1.0394; Test Acc: 86.4198\n",
      "\n",
      "Average Test Loss: 1.0394; Test Acc: 86.4198; Test mean IoU: 0.7690\n",
      "-------------------------------------------\n",
      "Epoch:[23/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0032\n",
      "Average Train Loss: 0.9934; Train Acc: 91.1175\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9917\n",
      "Average Test Loss: 1.0166; Test Acc: 88.7578\n",
      "\n",
      "Average Test Loss: 1.0166; Test Acc: 88.7578; Test mean IoU: 0.8047\n",
      "-------------------------------------------\n",
      "Epoch:[24/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9950\n",
      "Average Train Loss: 0.9927; Train Acc: 91.1907\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9815\n",
      "Average Test Loss: 1.1286; Test Acc: 77.2525\n",
      "\n",
      "Average Test Loss: 1.1286; Test Acc: 77.2525; Test mean IoU: 0.6396\n",
      "-------------------------------------------\n",
      "Epoch:[25/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9954\n",
      "Average Train Loss: 0.9899; Train Acc: 91.4609\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0516\n",
      "Average Test Loss: 1.0665; Test Acc: 83.6835\n",
      "\n",
      "Average Test Loss: 1.0665; Test Acc: 83.6835; Test mean IoU: 0.7266\n",
      "-------------------------------------------\n",
      "Epoch:[26/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9944\n",
      "Average Train Loss: 0.9887; Train Acc: 91.5777\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0221\n",
      "Average Test Loss: 1.0276; Test Acc: 87.6399\n",
      "\n",
      "Average Test Loss: 1.0276; Test Acc: 87.6399; Test mean IoU: 0.7872\n",
      "-------------------------------------------\n",
      "Epoch:[27/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9874\n",
      "Average Train Loss: 0.9956; Train Acc: 90.8891\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9705\n",
      "Average Test Loss: 1.0186; Test Acc: 88.5812\n",
      "\n",
      "Average Test Loss: 1.0186; Test Acc: 88.5812; Test mean IoU: 0.8038\n",
      "-------------------------------------------\n",
      "Epoch:[28/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9824\n",
      "Average Train Loss: 0.9900; Train Acc: 91.4535\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [341/341] Loss: 0.9743\n",
      "Average Test Loss: 1.0394; Test Acc: 86.4388\n",
      "\n",
      "Average Test Loss: 1.0394; Test Acc: 86.4388; Test mean IoU: 0.7681\n",
      "-------------------------------------------\n",
      "Epoch:[29/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9731\n",
      "Average Train Loss: 0.9891; Train Acc: 91.5403\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0750\n",
      "Average Test Loss: 1.0447; Test Acc: 85.9673\n",
      "\n",
      "Average Test Loss: 1.0447; Test Acc: 85.9673; Test mean IoU: 0.7620\n",
      "-------------------------------------------\n",
      "Epoch:[30/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9675\n",
      "Average Train Loss: 0.9866; Train Acc: 91.7819\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9811\n",
      "Average Test Loss: 1.0063; Test Acc: 89.7565\n",
      "\n",
      "Average Test Loss: 1.0063; Test Acc: 89.7565; Test mean IoU: 0.8223\n",
      "-------------------------------------------\n",
      "Epoch:[31/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9663\n",
      "Average Train Loss: 0.9855; Train Acc: 91.8878\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9962\n",
      "Average Test Loss: 1.0139; Test Acc: 89.0107\n",
      "\n",
      "Average Test Loss: 1.0139; Test Acc: 89.0107; Test mean IoU: 0.8095\n",
      "-------------------------------------------\n",
      "Epoch:[32/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0023\n",
      "Average Train Loss: 0.9851; Train Acc: 91.9294\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9938\n",
      "Average Test Loss: 1.0081; Test Acc: 89.6085\n",
      "\n",
      "Average Test Loss: 1.0081; Test Acc: 89.6085; Test mean IoU: 0.8202\n",
      "-------------------------------------------\n",
      "Epoch:[33/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9896\n",
      "Average Train Loss: 0.9842; Train Acc: 92.0210\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9723\n",
      "Average Test Loss: 1.0667; Test Acc: 83.6533\n",
      "\n",
      "Average Test Loss: 1.0667; Test Acc: 83.6533; Test mean IoU: 0.7263\n",
      "-------------------------------------------\n",
      "Epoch:[34/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9975\n",
      "Average Train Loss: 0.9827; Train Acc: 92.1773\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9881\n",
      "Average Test Loss: 1.0978; Test Acc: 80.3735\n",
      "\n",
      "Average Test Loss: 1.0978; Test Acc: 80.3735; Test mean IoU: 0.6827\n",
      "-------------------------------------------\n",
      "Epoch:[35/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0057\n",
      "Average Train Loss: 0.9812; Train Acc: 92.3279\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0304\n",
      "Average Test Loss: 1.1054; Test Acc: 79.6944\n",
      "\n",
      "Average Test Loss: 1.1054; Test Acc: 79.6944; Test mean IoU: 0.6728\n",
      "-------------------------------------------\n",
      "Epoch:[36/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9780\n",
      "Average Train Loss: 0.9810; Train Acc: 92.3560\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0056\n",
      "Average Test Loss: 1.2401; Test Acc: 66.1708\n",
      "\n",
      "Average Test Loss: 1.2401; Test Acc: 66.1708; Test mean IoU: 0.5139\n",
      "-------------------------------------------\n",
      "Epoch:[37/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9681\n",
      "Average Train Loss: 0.9831; Train Acc: 92.1403\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9915\n",
      "Average Test Loss: 1.0216; Test Acc: 88.2019\n",
      "\n",
      "Average Test Loss: 1.0216; Test Acc: 88.2019; Test mean IoU: 0.7959\n",
      "-------------------------------------------\n",
      "Epoch:[38/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9896\n",
      "Average Train Loss: 0.9796; Train Acc: 92.4912\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.2067\n",
      "Average Test Loss: 1.2672; Test Acc: 63.4408\n",
      "\n",
      "Average Test Loss: 1.2672; Test Acc: 63.4408; Test mean IoU: 0.4719\n",
      "-------------------------------------------\n",
      "Epoch:[39/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9754\n",
      "Average Train Loss: 0.9799; Train Acc: 92.4589\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9758\n",
      "Average Test Loss: 1.0153; Test Acc: 88.8811\n",
      "\n",
      "Average Test Loss: 1.0153; Test Acc: 88.8811; Test mean IoU: 0.8066\n",
      "-------------------------------------------\n",
      "Epoch:[40/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0006\n",
      "Average Train Loss: 0.9781; Train Acc: 92.6493\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0266\n",
      "Average Test Loss: 1.0512; Test Acc: 85.2014\n",
      "\n",
      "Average Test Loss: 1.0512; Test Acc: 85.2014; Test mean IoU: 0.7492\n",
      "-------------------------------------------\n",
      "Epoch:[41/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9800\n",
      "Average Train Loss: 0.9753; Train Acc: 92.9275\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9833\n",
      "Average Test Loss: 1.0108; Test Acc: 89.3280\n",
      "\n",
      "Average Test Loss: 1.0108; Test Acc: 89.3280; Test mean IoU: 0.8149\n",
      "-------------------------------------------\n",
      "Epoch:[42/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9835\n",
      "Average Train Loss: 0.9741; Train Acc: 93.0486\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9941\n",
      "Average Test Loss: 1.0143; Test Acc: 88.9468\n",
      "\n",
      "Average Test Loss: 1.0143; Test Acc: 88.9468; Test mean IoU: 0.8082\n",
      "-------------------------------------------\n",
      "Epoch:[43/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9670\n",
      "Average Train Loss: 0.9734; Train Acc: 93.1147\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9564\n",
      "Average Test Loss: 1.0031; Test Acc: 90.0706\n",
      "\n",
      "Average Test Loss: 1.0031; Test Acc: 90.0706; Test mean IoU: 0.8271\n",
      "-------------------------------------------\n",
      "Epoch:[44/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9736\n",
      "Average Train Loss: 0.9739; Train Acc: 93.0664\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9957\n",
      "Average Test Loss: 1.0020; Test Acc: 90.1999\n",
      "\n",
      "Average Test Loss: 1.0020; Test Acc: 90.1999; Test mean IoU: 0.8293\n",
      "-------------------------------------------\n",
      "Epoch:[45/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9595\n",
      "Average Train Loss: 0.9728; Train Acc: 93.1713\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0320\n",
      "Average Test Loss: 1.1141; Test Acc: 78.8268\n",
      "\n",
      "Average Test Loss: 1.1141; Test Acc: 78.8268; Test mean IoU: 0.6591\n",
      "-------------------------------------------\n",
      "Epoch:[46/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9687\n",
      "Average Train Loss: 0.9725; Train Acc: 93.1990\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9538\n",
      "Average Test Loss: 1.0030; Test Acc: 90.0933\n",
      "\n",
      "Average Test Loss: 1.0030; Test Acc: 90.0933; Test mean IoU: 0.8272\n",
      "-------------------------------------------\n",
      "Epoch:[47/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9967\n",
      "Average Train Loss: 0.9722; Train Acc: 93.2293\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0861\n",
      "Average Test Loss: 1.0887; Test Acc: 81.4496\n",
      "\n",
      "Average Test Loss: 1.0887; Test Acc: 81.4496; Test mean IoU: 0.6952\n",
      "-------------------------------------------\n",
      "Epoch:[48/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9694\n",
      "Average Train Loss: 0.9699; Train Acc: 93.4620\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9748\n",
      "Average Test Loss: 0.9976; Test Acc: 90.6402\n",
      "\n",
      "Average Test Loss: 0.9976; Test Acc: 90.6402; Test mean IoU: 0.8379\n",
      "-------------------------------------------\n",
      "Epoch:[49/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9858\n",
      "Average Train Loss: 0.9692; Train Acc: 93.5328\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9775\n",
      "Average Test Loss: 1.0482; Test Acc: 85.4401\n",
      "\n",
      "Average Test Loss: 1.0482; Test Acc: 85.4401; Test mean IoU: 0.7544\n",
      "-------------------------------------------\n",
      "Epoch:[50/50]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9670\n",
      "Average Train Loss: 0.9681; Train Acc: 93.6314\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9754\n",
      "Average Test Loss: 1.0023; Test Acc: 90.1622\n",
      "\n",
      "Average Test Loss: 1.0023; Test Acc: 90.1622; Test mean IoU: 0.8300\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "network = Segmentation(train_seg_dataset.num_classes, alignment=True).cuda()\n",
    "epochs = 50 # you can change the value to a small number for debugging\n",
    "# Training parameters are the same as the classiï¬cation network.\n",
    "## TASK 2.13\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)\n",
    "# # choose a lr scheduler\n",
    "# The learning rate is divided by 2 every 20 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "train_seg(train_seg_loader, test_seg_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test mIoU you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
